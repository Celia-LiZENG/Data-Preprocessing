{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------<Feature Selection>------------\n",
    "1. Notation\n",
    "    the process of selecting out the most significant features from a given datase\n",
    "\n",
    "2. Importance\n",
    "   It enables the machine learning algorithm to train faster.\n",
    "   It reduces the complexity of a model and makes it easier to interpret.\n",
    "   It improves the accuracy of a model if the right subset is chosen.\n",
    "   It reduces Overfitting.\n",
    "\n",
    "3. Topics\n",
    "    1) Difference between feature selection and dimensionality reduction\n",
    "    2) Different types of feature selection methods\n",
    "    3) Implementation of different feature selection methods with scikit-learn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----üî•Feature Selection VS. Dimensionality Reduction------\n",
    "1Ôºâ Both methods tend to reduce the number of attributes in the datasetÔºõ\n",
    "2Ôºâ Dimensionality reductionÔºöcreate new combinations of attributes (sometimes known as feature transformation),  \n",
    "Example: Principal Component Analysis, Singular Value Decomposition, Linear Discriminant Analysis,\n",
    "3)  feature selection: include and exclude attributes present in the data without changing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3 Methods: Filter methods, Wrapper methods, and Embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------Method1: Filter Method (data preprocessing step)--------------\n",
    "#Filter out irrelevant features before classification process starts.\n",
    " ‚ú®give rank on the basis of statistical scores which tend to determine the features' correlation with the outcome variableÔºõ\n",
    " ‚ú®Features give rank on the basis of statistical scores which tend to determine the features' correlation with the outcome variable.\n",
    " ‚ö†Ô∏è correlation coefficients for different types of data\n",
    "    \n",
    "    Feature/Response     Continuous                Categorical\n",
    "           Continous     Pearson's Correlation     LDA\n",
    "           Categorical   Anova                     Chi-Squire\n",
    "       \n",
    "       Assumption:\n",
    "         1ÔºâPearson's Correlation: independence, linearity \n",
    "         \n",
    " ‚ö†Ô∏èExamples: Chi-squared test, information gain, and correlation coefficient scores\n",
    "\n",
    "---------Method2: Wrapper Method--------------\n",
    " ‚ú®searches for a feature: best-suited for the machine learning algorithm and aims to improve the mining performance.  To evaluate the features, the predictive accuracy used for classification tasks and goodness of cluster is evaluated using clustering.\n",
    " \n",
    " üî•Examples:\n",
    " 1) Forward Selection: The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set.\n",
    "\n",
    "2) Backward Elimination: The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set.\n",
    "\n",
    "3) Combination of forward selection and backward elimination: \n",
    "The stepwise forward selection and backward elimination methods can be combined so that, at each step, the procedure selects the best attribute and removes the worst from among the remaining attributes.\n",
    "\n",
    "4) Recursive Feature elimination: a greedy search to find the best performing feature subset. \n",
    "   1. Iteratively creates models and determines the best or the worst performing feature (eliminate) at each iteration. \n",
    "   2. Constructs the subsequent models with the left features until all the features are explored.\n",
    "   3. Then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2^N combinations of features.\n",
    " \n",
    "----------Method3: Embedded Method------------------\n",
    "1ÔºâTakes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iterationÔºõ\n",
    "2ÔºâIntroduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients)\n",
    "3ÔºâExampleÔºöRegularization methods Ôºàpenalization method)\n",
    "          --> penalize a feature given a coefficient threshold\n",
    "          i.e., LASSO, \n",
    "          Elastic Net, \n",
    "          Ridge Regression(create a parsimonious model when the number of predictor variables in a set exceeds the number of observations, or when a data set has multicollinearity.\n",
    "          \n",
    "          \n",
    "-----------Extra: Some thinking about those methods‚ùì---------\n",
    "filter method: \n",
    "       1) not incorporate ML; \n",
    "       2) may fail to find the best subset of features in situations when there is not enough data to model the statistical correlation of the features\n",
    "       \n",
    "wrapper method:\n",
    "       1) Include ML --> may lead to overfitting\n",
    "       2) Can always provide the best subset of features despite insufficient data;\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------Case Study in Python-----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6. , 148. ,  72. , ...,   0. ,  33.6,  50. ],\n",
       "       [  1. ,  85. ,  66. , ...,   0. ,  26.6,  31. ],\n",
       "       [  8. , 183. ,  64. , ...,   0. ,  23.3,  32. ],\n",
       "       ...,\n",
       "       [  5. , 121. ,  72. , ..., 112. ,  26.2,  30. ],\n",
       "       [  1. , 126. ,  60. , ...,   0. ,  30.1,  47. ],\n",
       "       [  1. ,  93. ,  70. , ...,   0. ,  30.4,  23. ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------Method1:Filter Method-----------\n",
    "#-----1.1. Use Variance------\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "VarianceThreshold(threshold=3).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n",
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "#------1.2. Use Coefficient--------\n",
    "#SelectKBest class that can be used with a suite of different statistical tests to \n",
    "# --> select a specific number of features\n",
    "# Import the necessary libraries first\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#Test1: User Pearson\n",
    "test1 = SelectKBest(score_func=pearsonr, k=4)\n",
    "fit1 = test.fit(X, Y)\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit1.scores_)\n",
    "features_T1 = fit1.transform(X)\n",
    "# Summarize selected features\n",
    "print(features_T1[0:5,:])\n",
    "\n",
    "#Test2: Use Chi-squire \n",
    "test2 = SelectKBest(score_func=chi2, k=4)\n",
    "fit2 = test2.fit(X, Y)\n",
    "\n",
    "# Summarize scores for each attribute\n",
    "print(fit2.scores_)\n",
    "\n",
    "features_T2 = fit2.transform(X)\n",
    "# Summarize selected features\n",
    "print(features_T2[0:5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'minepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ab0e88eba13d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#‚ö†Ô∏èEvaluate the correlation between categorical X and categorical Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mminepy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'minepy'"
     ]
    }
   ],
   "source": [
    "#Test3: Multual Information\n",
    "#‚ö†Ô∏èEvaluate the correlation between categorical X and categorical Y\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from minepy import MINE\n",
    "def mic(x, y):\n",
    "     m = MINE()\n",
    "     m.compute_score(x, y)\n",
    "     return (m.mic(), 0.5) #0.5: fixed p-value\n",
    "test3 = SelectKBest(score_func=mic, k=4)\n",
    "fit3 = test3.fit(X, Y)\n",
    "\n",
    "print(fit3.scores_)\n",
    "features_T3 = fit3.transform(X)\n",
    "# Summarize selected features\n",
    "print(features_T3[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/celiahah/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/celiahah/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/celiahah/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/celiahah/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/celiahah/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/celiahah/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#--------------Method2:Wrapper Methods----------\n",
    "#1. Recursive Feature Elimination \n",
    "#How to work: \n",
    "#(1)Recursively removing attributes and building a model on those attributes that remain\n",
    "#(2)uses the model accuracy to identify which attributes \n",
    "# (and combination of attributes) contribute the most to predicting the target attribute\n",
    "\n",
    "# Import your necessary dependencies\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % (fit.support_))\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
    "#üî•Top 3 features as preg, mass, and pedi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----Method3: Embeded Method-------\n",
    "# Try: Ridge regression which is basically a regularization technique\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X,Y)\n",
    "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "   normalize=False, random_state=None, solver='auto', tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge model: 0.021 * X0 + 0.006 * X1 + -0.002 * X2 + 0.0 * X3 + -0.0 * X4 + 0.013 * X5 + 0.145 * X6 + 0.003 * X7\n"
     ]
    }
   ],
   "source": [
    "# A helper method for pretty-printing the coefficients\n",
    "def pretty_print_coefs(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    "\n",
    "print (\"Ridge model:\", pretty_print_coefs(ridge.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/celiahah/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#More Thoughts about Embeded Methods: L1 and L2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Use Logistic Regression with L1 for Feature Selection:\n",
    "Selection1=SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(X, Y)\n",
    "\n",
    "#Use L1+L2 to construct a new LR model:\n",
    "class LR(LogisticRegression):\n",
    "    def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0,\n",
    "                 fit_intercept=True, intercept_scaling=1, class_weight=None,\n",
    "                 random_state=None, solver='liblinear', max_iter=100,\n",
    "                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):\n",
    "        self.threshold = threshold\n",
    "        LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C,\n",
    "                 fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight,\n",
    "                 random_state=random_state, solver=solver, max_iter=max_iter,\n",
    "                 multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "\n",
    "def fit(self, X, y, sample_weight=None):\n",
    "        super(LR, self).fit(X, y, sample_weight=sample_weight)\n",
    "        self.coef_old_ = self.coef_.copy()\n",
    "        self.l2.fit(X, y, sample_weight=sample_weight)\n",
    "        cntOfRow, cntOfCol = self.coef_.shape\n",
    "\n",
    "        for i in range(cntOfRow):\n",
    "            for j in range(cntOfCol):\n",
    "                coef = self.coef_[i][j]\n",
    "                if coef != 0:\n",
    "                    idx = [j]\n",
    "                    coef1 = self.l2.coef_[i][j]\n",
    "                    for k in range(cntOfCol):\n",
    "                        coef2 = self.l2.coef_[i][k]\n",
    "                        # in L2, the difference between threshold + self.coef in L1 is 0\n",
    "                        if abs(coef1-coef2) < self.threshold and j != k and self.coef_[i][k] == 0:\n",
    "                            idx.append(k)\n",
    "                    mean = coef / len(idx)\n",
    "                    self.coef_[i][idx] = mean\n",
    "        return self\n",
    "\n",
    "#threshold is the difference between two coefficients        \n",
    "Selection2=SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#‚ö†Ô∏èSome tips when applying Ridege Regression:\n",
    "1. It is also known as L2-Regularization.\n",
    "2. For correlated features, it means that they tend to get similar coefficients.\n",
    "3. Feature having negative coefficients don't contribute that much. But in a more complex scenario where you are dealing with lots of features, then this score will definitely help you in the ultimate feature selection decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148. ,  33.6,  50. ],\n",
       "       [ 85. ,  26.6,  31. ],\n",
       "       [183. ,  23.3,  32. ],\n",
       "       ...,\n",
       "       [121. ,  26.2,  30. ],\n",
       "       [126. ,  30.1,  47. ],\n",
       "       [ 93. ,  30.4,  23. ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----3.3.Based on Tree-----\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Use GBDT\n",
    "SelectFromModel(GradientBoostingClassifier()).fit_transform(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------üî•4. Addition: Decreasing Dimention with PCA + LDA--------\n",
    "from sklearn.decomposition import PCA\n",
    "PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "from sklearn.lda import LDA\n",
    "LDA(n_components=2).fit_transform(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "1. DataCamp: Beginner's Guide to Feature Selection in Python\n",
    "https://www.datacamp.com/community/tutorials/feature-selection-python\n",
    "2. Buntine, Wray (et al.), Subspace, Latent Structure, and Feature Selection: Statistical and Optimization Perspectives Workshop\n",
    "3. Feature Selection using Genetic Algorithms: https://topepo.github.io/caret/feature-selection-using-genetic-algorithms.html\n",
    "4.  https://www.zhihu.com/question/28641663/answer/139203996"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
